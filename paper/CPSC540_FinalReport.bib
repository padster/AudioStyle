@article{Gatys2015,
abstract = {In fine art, especially painting, humans have mastered the skill to create unique visual experiences through composing a complex interplay between the con- tent and style of an image. Thus far the algorithmic basis of this process is unknown and there exists no artificial system with similar capabilities. How- ever, in other key areas of visual perception such as object and face recognition near-human performance was recently demonstrated by a class of biologically inspired vision models called Deep Neural Networks.1,2 Here we introduce an artificial system based on a Deep Neural Network that creates artistic images of high perceptual quality. The system uses neural representations to sepa- rate and recombine content and style of arbitrary images, providing a neural algorithm for the creation of artistic images. Moreover, in light of the strik- ing similarities between performance-optimised artificial neural networks and biological vision,3?7 our work offers a path forward to an algorithmic under- standing of how humans create and perceive artistic imagery.},
annote = {NULL},
archivePrefix = {arXiv},
arxivId = {1508.06576},
author = {Gatys, Leon A and Ecker, Alexander S and Bethge, Matthias},
doi = {10.1561/2200000006},
eprint = {1508.06576},
file = {:Users/adamporisky/Documents/School/Classes/CPSC540/Project/Lit Review/1508.06576.pdf:pdf},
isbn = {2200000006},
issn = {1935-8237},
journal = {arXiv preprint},
keywords = {eural algorithm of artistic,style},
mendeley-groups = {CPSC540 Project},
pages = {3--7},
pmid = {1000200972},
title = {{A Neural Algorithm of Artistic Style}},
year = {2015}
}

@article{Choi2016,
  author    = {Keunwoo Choi and
               George Fazekas and
               Mark B. Sandler and
               Kyunghyun Cho},
  title     = {Convolutional Recurrent Neural Networks for Music Classification},
  journal   = {CoRR},
  volume    = {abs/1609.04243},
  year      = {2016},
  url       = {http://arxiv.org/abs/1609.04243},
  timestamp = {Mon, 03 Oct 2016 17:51:10 +0200},
  biburl    = {http://dblp.uni-trier.de/rec/bib/journals/corr/ChoiFSC16},
  bibsource = {dblp computer science bibliography, http://dblp.org}
}

@inproceedings{Tzanetakis2001,
abstract = {Musical genres are categorical descriptions that are used to describe$\backslash$nmusic. They are commonly used to structure the increasing amounts$\backslash$nof music available in digital form on the Web and are important for$\backslash$nmusic information retrieval. Genre categorization for audio has traditionally$\backslash$nbeen performed manually. A particular musical genre is characterized$\backslash$nby statistical properties related to the instrumentation, rhythmic$\backslash$nstructure and form of its members. In this work, algorithms for the$\backslash$nautomatic genre categorization of audio signals are described. More$\backslash$nspecifically, we propose a set of features for representing texture$\backslash$nand instrumentation. In addition a novel set of features for representing$\backslash$nrhythmic structure and strength is proposed. The performance of those$\backslash$nfeature sets has been evaluated by training statistical pattern recognition$\backslash$nclassifiers using real world audio collections. Based on the automatic$\backslash$nhierarchical genre classification two graphical user interfaces for$\backslash$nbrowsing and interacting with large audio collections have been developed.},
author = {Tzanetakis, George and Essl, Georg and Cook, Perry},
booktitle = {ISMIR},
doi = {http://ismir2001.ismir.net/pdf/tzanetakis.pdf},
mendeley-groups = {CPSC540 Project},
title = {{Automatic musical genre classification of audio signals}},
year = {2001}
}

@misc{nsynth2017,
    Author = {Jesse Engel and Cinjon Resnick and Adam Roberts and
              Sander Dieleman and Douglas Eck and Karen Simonyan and
              Mohammad Norouzi},
    Title = {Neural Audio Synthesis of Musical Notes with WaveNet Autoencoders},
    Year = {2017},
    Eprint = {arXiv:1704.01279},
}

@article{Feng2014,
abstract = {In this paper we will present how to use Re- stricted Boltzmann machine algorithm to build deep belief neural networks.The goal is to use it to perform a multi-class classification task of labelling music genres and compare it to that of the vanilla neural networks. We expected that deep learning would out-perform how- ever the results we obtained for 2 and 3 class classification turn out to be on par for deep neural networks and the vanilla version with small data set. By generating more dataset from the original limited music tracks, we see a great classification accuracy improvement in the deep belief neural network and its out- performance than neural networks. 1},
author = {Feng, Tao},
file = {:Users/adamporisky/Documents/School/Classes/CPSC540/Project/Lit Review/drive-download-20170327T161515Z-001/Deep Learning for Music Genre.pdf:pdf},
journal = {Courses.Engr.Illinois.Edu},
mendeley-groups = {CPSC540 Project},
number = {1},
title = {{Deep learning for music genre classification}},
url = {https://courses.engr.illinois.edu/ece544na/fa2014/Tao{\_}Feng.pdf},
year = {2014}
}

@article{Li2010,
abstract = {Music genre classification has been a challenging yet promising task in the field of music information retrieval (MIR). Due to the highly elusive characteristics of audio musical data, retrieving informative and reliable features from audio signals is crucial to the performance of any music genre classification system. Previous work on audio music genre classification systems mainly concentrated on using timbral features, which limits the performance. To address this problem, we propose a novel approach to extract musical pattern features in audio music using convolutional neural network (CNN), a model widely adopted in image information retrieval tasks. Our experiments show that CNN has strong capacity to capture informative features from the variations of musical patterns with minimal prior knowledge provided.},
author = {Li, Tom L H and Chan, Antoni B and Chun, Andy H W},
doi = {10.1.1.302.7795},
file = {:Users/adamporisky/Documents/School/Classes/CPSC540/Project/Lit Review/drive-download-20170327T161515Z-001/Automatic Musical Pattern Feature Extraction.pdf:pdf},
isbn = {9789881701282},
journal = {Proceedings of the International MultiConference of Engineers and Computer Scientists (IMECS 2010)},
keywords = {convolutional neural network,multime,music feature extractor,music informa,tion retrieval},
mendeley-groups = {CPSC540 Project},
pages = {546--550},
title = {{Automatic Musical Pattern Feature Extraction Using Convolutional Neural Network}},
url = {http://posterous.com/getfile/files.posterous.com/troylee/7ke4ifkuea4av7Rcp38LhGhTALJOZcdAK3PXuAGDhamoyFikjkqPdxw52bwI/IMECS2010{\_}pp546-550.pdf},
volume = {I},
year = {2010}
}

@article{Sahi2012,
 author = {Sahidullah, Md. and Saha, Goutam},
 title = {Design, Analysis and Experimental Evaluation of Block Based Transformation in MFCC Computation for Speaker Recognition},
 journal = {Speech Commun.},
 issue_date = {May, 2012},
 volume = {54},
 number = {4},
 month = may,
 year = {2012},
 issn = {0167-6393},
 pages = {543--565},
 numpages = {23},
 url = {http://dx.doi.org/10.1016/j.specom.2011.11.004},
 doi = {10.1016/j.specom.2011.11.004},
 acmid = {2134080},
 publisher = {Elsevier Science Publishers B. V.},
 address = {Amsterdam, The Netherlands, The Netherlands},
 keywords = {Block transform, Correlation matrix, DCT, Decorrelation technique, Linear transformation, MFCC, Missing feature theory, Narrow-band noise, Speaker recognition},
} 

@article{Logan2000,
abstract = {We examine in some detail Mel Frequency Cepstral Coefficients (MFCCs) - the dominant features used for speech recognition - and investigate their applicability to modeling music. In particular, we examine two of the main assumptions of the process of forming MFCCs: the use of the Mel frequency scale to model the spectra; and the use of the Discrete Cosine Transform (DCT) to decorrelate the Mel-spectral vectors.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Logan, Beth},
doi = {10.1.1.11.9216},
eprint = {arXiv:1011.1669v3},
isbn = {9788578110796},
issn = {1098-6596},
journal = {International Symposium on Music Information Retrieval},
pages = {11p.},
pmid = {25246403},
title = {{Mel Frequency Cepstral Coefficients for Music Modeling}},
url = {http://ismir2000.ismir.net/papers/logan{\_}paper.pdf},
volume = {28},
year = {2000}
}

@article{Tzanetakis2002,
abstract = { Musical genres are categorical labels created by humans to characterize pieces of music. A musical genre is characterized by the common characteristics shared by its members. These characteristics typically are related to the instrumentation, rhythmic structure, and harmonic content of the music. Genre hierarchies are commonly used to structure the large collections of music available on the Web. Currently musical genre annotation is performed manually. Automatic musical genre classification can assist or replace the human user in this process and would be a valuable addition to music information retrieval systems. In addition, automatic musical genre classification provides a framework for developing and evaluating features for any type of content-based analysis of musical signals. In this paper, the automatic classification of audio signals into an hierarchy of musical genres is explored. More specifically, three feature sets for representing timbral texture, rhythmic content and pitch content are proposed. The performance and relative importance of the proposed features is investigated by training statistical pattern recognition classifiers using real-world audio collections. Both whole file and real-time frame-based classification schemes are described. Using the proposed feature sets, classification of 61{\%} for ten musical genres is achieved. This result is comparable to results reported for human musical genre classification.},
author = {Tzanetakis, George and Cook, Perry},
doi = {10.1109/TSA.2002.800560},
isbn = {1063-6676},
issn = {10636676},
journal = {IEEE Transactions on Speech and Audio Processing},
keywords = {Audio classification,Beat analysis,Feature extraction,Musical genre classification,Wavelets},
number = {5},
pages = {293--302},
pmid = {7359731},
title = {{Musical genre classification of audio signals}},
volume = {10},
year = {2002}
}


\end